{"cells":[{"cell_type":"markdown","metadata":{"id":"6ImLMYNPP6Ja"},"source":["<a \n","href=\"https://colab.research.google.com/github/wingated/cs474_labs_f2019/blob/master/DL_Lab6.ipynb\"\n","  target=\"_parent\">\n","  <img\n","    src=\"https://colab.research.google.com/assets/colab-badge.svg\"\n","    alt=\"Open In Colab\"/>\n","</a>"]},{"cell_type":"markdown","metadata":{"id":"cksgAH12XRjV"},"source":["# Lab 6: Sequence-to-sequence models\n","\n","### Description:\n","For this lab, you will code up the [char-rnn model of Karpathy](http://karpathy.github.io/2015/05/21/rnn-effectiveness/). This is a recurrent neural network that is trained probabilistically on sequences of characters, and that can then be used to sample new sequences that are like the original.\n","\n","This lab will help you develop several new skills, as well as understand some best practices needed for building large models. In addition, we'll be able to create networks that generate neat text!\n","\n","### Deliverable:\n","- Fill in the code for the RNN (using PyTorch's built-in GRU).\n","- Fill in the training loop\n","- Fill in the evaluation loop. In this loop, rather than using a validation set, you will sample text from the RNN.\n","- Implement your own GRU cell.\n","- Train your RNN on a new domain of text (Star Wars, political speeches, etc. - have fun!)\n","\n","### Grading Standards:\n","- 20% Implementation the RNN\n","- 20% Implementation training loop\n","- 20% Implementation of evaluation loop\n","- 20% Implementation of your own GRU cell\n","- 20% Training of your RNN on a domain of your choice\n","\n","### Tips:\n","- Read through all the helper functions, run them, and make sure you understand what they are doing\n","- At each stage, ask yourself: What should the dimensions of this tensor be? Should its data type be float or int? (int is called `long` in PyTorch)\n","- Don't apply a softmax inside the RNN if you are using an nn.CrossEntropyLoss (this module already applies a softmax to its input).\n","\n","### Example Output:\n","An example of my final samples are shown below (more detail in the\n","final section of this writeup), after 150 passes through the data.\n","Please generate about 15 samples for each dataset.\n","\n","<code>\n","And ifte thin forgision forward thene over up to a fear not your\n","And freitions, which is great God. Behold these are the loss sub\n","And ache with the Lord hath bloes, which was done to the holy Gr\n","And appeicis arm vinimonahites strong in name, to doth piseling \n","And miniquithers these words, he commanded order not; neither sa\n","And min for many would happine even to the earth, to said unto m\n","And mie first be traditions? Behold, you, because it was a sound\n","And from tike ended the Lamanites had administered, and I say bi\n","</code>\n"]},{"cell_type":"markdown","metadata":{"id":"c2i_QpSsWG4c"},"source":["---\n","\n","## Part 0: Readings, data loading, and high level training\n","\n","---\n","\n","There is a tutorial here that will help build out scaffolding code, and get an understanding of using sequences in pytorch.\n","\n","* Read the following\n","\n","> * [Pytorch sequence-to-sequence tutorial](https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html) (You will be implementing the decoder, not the encoder, as we are not doing sequence-to-sequence translation.)\n","* [Understanding LSTM Networks](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)\n","\n","\n","\n","\n"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"l7bdZWxvJrsx","executionInfo":{"status":"ok","timestamp":1665967568466,"user_tz":360,"elapsed":10400,"user":{"displayName":"Grant White","userId":"00068657515062458882"}},"outputId":"35d4df37-0115-4ff5-b72c-02cfda501dd6"},"outputs":[{"output_type":"stream","name":"stdout","text":["--2022-10-17 00:45:57--  https://piazza.com/redirect/s3?bucket=uploads&prefix=attach%2Fjlifkda6h0x5bk%2Fhzosotq4zil49m%2Fjn13x09arfeb%2Ftext_files.tar.gz\n","Resolving piazza.com (piazza.com)... 54.210.151.85, 52.200.164.228, 18.207.91.3, ...\n","Connecting to piazza.com (piazza.com)|54.210.151.85|:443... connected.\n","HTTP request sent, awaiting response... 302 Found\n","Location: https://cdn-uploads.piazza.com/attach/jlifkda6h0x5bk/hzosotq4zil49m/jn13x09arfeb/text_files.tar.gz [following]\n","--2022-10-17 00:45:58--  https://cdn-uploads.piazza.com/attach/jlifkda6h0x5bk/hzosotq4zil49m/jn13x09arfeb/text_files.tar.gz\n","Resolving cdn-uploads.piazza.com (cdn-uploads.piazza.com)... 13.226.228.46, 13.226.228.50, 13.226.228.49, ...\n","Connecting to cdn-uploads.piazza.com (cdn-uploads.piazza.com)|13.226.228.46|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 1533290 (1.5M) [application/x-gzip]\n","Saving to: ‘./text_files.tar.gz’\n","\n","./text_files.tar.gz 100%[===================>]   1.46M  2.04MB/s    in 0.7s    \n","\n","2022-10-17 00:45:59 (2.04 MB/s) - ‘./text_files.tar.gz’ saved [1533290/1533290]\n","\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting unidecode\n","  Downloading Unidecode-1.3.6-py3-none-any.whl (235 kB)\n","\u001b[K     |████████████████████████████████| 235 kB 4.3 MB/s \n","\u001b[?25hInstalling collected packages: unidecode\n","Successfully installed unidecode-1.3.6\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (1.12.1+cu113)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch) (4.1.1)\n","file_len = 2579888\n"]}],"source":["! wget -O ./text_files.tar.gz 'https://piazza.com/redirect/s3?bucket=uploads&prefix=attach%2Fjlifkda6h0x5bk%2Fhzosotq4zil49m%2Fjn13x09arfeb%2Ftext_files.tar.gz' \n","! tar -xzf text_files.tar.gz\n","! pip install unidecode\n","! pip install torch\n","\n","import unidecode\n","import string\n","import random\n","import re\n"," \n","import pdb\n"," \n","all_characters = string.printable\n","n_characters = len(all_characters)\n","file = unidecode.unidecode(open('./text_files/lotr.txt').read())\n","file_len = len(file)\n","print('file_len =', file_len)"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TxBeKeNjJ0NQ","executionInfo":{"status":"ok","timestamp":1665967588972,"user_tz":360,"elapsed":5,"user":{"displayName":"Grant White","userId":"00068657515062458882"}},"outputId":"48b06557-d5da-483d-a6b0-a7081868b195"},"outputs":[{"output_type":"stream","name":"stdout","text":["will.\n","hopefully it will.  how come?\n","how come?   i like how clear the sky gets after it rains.\n","i like how clear the sky gets after it rains.   i feel the same way. it smells so good after it rains.\n","i fe\n"]}],"source":["chunk_len = 200\n"," \n","def random_chunk():\n","    start_index = random.randint(0, file_len - chunk_len)\n","    end_index = start_index + chunk_len + 1\n","    return file[start_index:end_index]\n","  \n","print(random_chunk())"]},{"cell_type":"code","source":["import torch"],"metadata":{"id":"udbHqUkXQSE9","executionInfo":{"status":"ok","timestamp":1665967599441,"user_tz":360,"elapsed":2058,"user":{"displayName":"Grant White","userId":"00068657515062458882"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"On0_WitWJ99e","executionInfo":{"status":"ok","timestamp":1665967599441,"user_tz":360,"elapsed":2,"user":{"displayName":"Grant White","userId":"00068657515062458882"}},"outputId":"4318f687-c9eb-4ba1-b558-70e005d4eaef"},"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([10, 11, 12, 39, 40, 41])\n"]}],"source":["# Turn string into list of longs\n","def char_tensor(string):\n","    tensor = torch.zeros(len(string)).long()\n","    for c in range(len(string)):\n","        tensor[c] = all_characters.index(string[c])\n","    return tensor\n","\n","print(char_tensor('abcDEF'))"]},{"cell_type":"markdown","metadata":{"id":"CYJPTLcaYmfI"},"source":["---\n","\n","## Part 4: Creating your own GRU cell \n","\n","**(Come back to this later - its defined here so that the GRU will be defined before it is used)**\n","\n","---\n","\n","The cell that you used in Part 1 was a pre-defined Pytorch layer. Now, write your own GRU class using the same parameters as the built-in Pytorch class does.\n","\n","Please do not look at the documentation's code for the GRU cell definition. The answer is right there in the code, and in theory, you could just cut-and-paste it. This bit is on your honor!\n","\n","**TODO:**\n","\n","**DONE:**\n","* Create a custom GRU cell\n","\n"]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F"],"metadata":{"id":"yKU4XdQFP7PR","executionInfo":{"status":"ok","timestamp":1665967603670,"user_tz":360,"elapsed":925,"user":{"displayName":"Grant White","userId":"00068657515062458882"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aavAv50ZKQ-F"},"outputs":[],"source":["class GRU(nn.Module):\n","    def __init__(self, input_size, hidden_size, num_layers):\n","        super(GRU, self).__init__()\n","        self.lin1 = nn.Linear(input_size, hidden_size)\n","        self.lin2 = nn.Linear(hidden_size, hidden_size)\n","        self.lin3 = nn.Linear(input_size, hidden_size)\n","        self.lin4 = nn.Linear(hidden_size, hidden_size)\n","        self.lin5 = nn.Linear(input_size, hidden_size)\n","        self.lin6 = nn.Linear(hidden_size, hidden_size)\n","\n","    def forward(self, inputs, hiddens):\n","        # Each layer does the following:\n","        # r_t = sigmoid(W_ir*x_t + b_ir + W_hr*h_(t-1) + b_hr)\n","        # z_t = sigmoid(W_iz*x_t + b_iz + W_hz*h_(t-1) + b_hz)\n","        # n_t = tanh(W_in*x_t + b_in + r_t**(W_hn*h_(t-1) + b_hn))\n","        # h_(t) = (1 - z_t)**n_t + z_t**h_(t-1)\n","        # Where ** is hadamard product (not matrix multiplication, but elementwise multiplication)\n","\n","        h_t_1 = hiddens[-1]\n","        r_t = F.sigmoid(self.lin1(inputs) + self.lin2(h_t_1))\n","        z_t = F.sigmoid(self.lin3(inputs) + self.lin4(h_t_1))\n","        n_t = F.tanh(self.lin5(inputs) + (r_t * self.lin6(h_t_1)))\n","        h_t = ((1 - z_t) * n_t) + (z_t * h_t_1)\n","\n","        return h_t, hiddens.append(h_t)\n","\n","        # r_t = F.sigmoid(self.lin1(inputs) + self.lin2(hiddens))\n","        # z_t = F.sigmoid(self.lin3(inputs) + self.lin4(hiddens))\n","        # n_t = F.tanh(self.lin5(inputs) + (r_t * self.lin6(hiddens)))\n","        # h_t = ((1 - z_t) * n_t) + (z_t * hiddens)\n","\n","        # return h_t"]},{"cell_type":"markdown","metadata":{"id":"qtXdX-B_WiAY"},"source":["---\n","\n","##  Part 1: Building a sequence to sequence model\n","\n","---\n","\n","Great! We have the data in a useable form. We can switch out which text file we are reading from, and trying to simulate.\n","\n","We now want to build out an RNN model, in this section, we will use all built in Pytorch pieces when building our RNN class.\n","\n","\n","**TODO:**\n","\n","**DONE:**\n","* Create an RNN class that extends from nn.Module.\n","\n"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"d6tNdEnzWj5F","executionInfo":{"status":"ok","timestamp":1665967609226,"user_tz":360,"elapsed":290,"user":{"displayName":"Grant White","userId":"00068657515062458882"}}},"outputs":[],"source":["class RNN(nn.Module):\n","    def __init__(self, input_size, hidden_size, output_size, n_layers=1):\n","        super(RNN, self).__init__()\n","        self.input_size = input_size\n","        self.hidden_size = hidden_size\n","        self.output_size = output_size\n","        self.n_layers = n_layers\n","\n","        # more stuff here...\n","        self.embedding = nn.Embedding(input_size, hidden_size)\n","        self.gru = nn.GRU(hidden_size, hidden_size, n_layers)\n","        self.out = nn.Linear(hidden_size, output_size)\n","\n","    def forward(self, input_char, hidden):\n","        # by reviewing the documentation, construct a forward function that properly uses the output\n","        # of the GRU\n","\n","        # stuff here\n","        output = F.relu(self.embedding(input_char).view(1,1,-1))\n","        # output.unsqueeze_(0).unsqueeze_(0)\n","        output, hidden = self.gru(output, hidden)\n","        output = self.out(output)\n","        return output, hidden\n","\n","    def init_hidden(self):\n","        return torch.zeros(self.n_layers, 1, self.hidden_size)"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"hrhXghEPKD-5","executionInfo":{"status":"ok","timestamp":1665967611149,"user_tz":360,"elapsed":3,"user":{"displayName":"Grant White","userId":"00068657515062458882"}}},"outputs":[],"source":["def random_training_set():    \n","    chunk = random_chunk()\n","    inp = char_tensor(chunk[:-1])\n","    target = char_tensor(chunk[1:])\n","    return inp, target"]},{"cell_type":"markdown","metadata":{"id":"ZpiGObbBX0Mr"},"source":["---\n","\n","## Part 2: Sample text and Training information\n","\n","---\n","\n","We now want to be able to train our network, and sample text after training.\n","\n","This function outlines how training a sequence style network goes. \n","\n","**TODO:**\n","\n","**DONE:**\n","* Fill in the pieces.\n","\n","\n"]},{"cell_type":"code","execution_count":14,"metadata":{"id":"2ALC3Pf8Kbsi","executionInfo":{"status":"ok","timestamp":1665967919906,"user_tz":360,"elapsed":2,"user":{"displayName":"Grant White","userId":"00068657515062458882"}}},"outputs":[],"source":["# NOTE: decoder_optimizer, decoder, and criterion will be defined below as global variables\n","def train(inp, target):\n","    ## initialize hidden layers, set up gradient and loss \n","    # your code here\n","    objective = nn.CrossEntropyLoss()\n","    decoder_optimizer.zero_grad()\n","    hidden = decoder.init_hidden()\n","    hidden = hidden.cuda()\n","    loss = 0\n","\n","    # more stuff here...\n","\n","    # calculate loss\n","    for i, c in enumerate(inp):\n","        c = c.cuda()\n","        y_hat, hidden = decoder(c, hidden)\n","        y_hat.squeeze_()\n","        y_truth = target[i].cuda()\n","        loss += objective(y_hat, y_truth)\n","\n","    # calculate gradients\n","    with torch.autograd.set_detect_anomaly(True):\n","        loss.backward()\n","\n","    # step the optimizer\n","    decoder_optimizer.step()\n","\n","    # return average loss\n","    return loss / len(target)"]},{"cell_type":"markdown","metadata":{"id":"EN06NUu3YRlz"},"source":["---\n","\n","## Part 3: Sample text and Training information\n","\n","---\n","\n","You can at this time, if you choose, also write out your train loop boilerplate that samples random sequences and trains your RNN. This will be helpful to have working before writing your own GRU class.\n","\n","If you are finished training, or during training, and you want to sample from the network you may consider using the following function. If your RNN model is instantiated as `decoder`then this will probabilistically sample a sequence of length `predict_len`\n","\n","**TODO:**\n","\n","**DONE:**\n","* Fill out the evaluate function to generate text from a primed string\n","\n"]},{"cell_type":"code","execution_count":15,"metadata":{"id":"B-bp-OZ1KjNh","executionInfo":{"status":"ok","timestamp":1665967932270,"user_tz":360,"elapsed":537,"user":{"displayName":"Grant White","userId":"00068657515062458882"}}},"outputs":[],"source":["def sample_outputs(output, temperature):\n","    \"\"\"Takes in a vector of unnormalized probability weights and samples a character from the distribution\"\"\"\n","    # As temperature approaches 0, this sampling function becomes argmax (no randomness)\n","    # As temperature approaches infinity, this sampling function becomes a purely random choice\n","    return torch.multinomial(torch.exp(output / temperature), 1)\n","\n","def evaluate(prime_str='A', predict_len=100, temperature=0.8):\n","    ## initialize hidden state, initialize other useful variables\n","    # your code here\n","\n","    with torch.inference_mode():\n","        # initialize hidden\n","        hidden = decoder.init_hidden()\n","        hidden = hidden.cuda()\n","\n","        n = len(prime_str)\n","        for i in range(predict_len):\n","            # prime string\n","            if i < n:\n","                c = char_tensor(prime_str[i])\n","                c = c.cuda()\n","                hidden = decoder(c, hidden)[1]\n","\n","            # prediction\n","            else:\n","                c = char_tensor(prime_str[-1])\n","                c = c.cuda()\n","                output, hidden = decoder(c, hidden)\n","                new_c = sample_outputs(output.squeeze_(), temperature)\n","                prime_str += all_characters[new_c]\n","\n","    return prime_str"]},{"cell_type":"markdown","metadata":{"id":"Du4AGA8PcFEW"},"source":["---\n","\n","## Part 4: (Create a GRU cell, requirements above)\n","\n","---\n"]},{"cell_type":"markdown","metadata":{"id":"GFS2bpHSZEU6"},"source":["\n","---\n","\n","## Part 5: Run it and generate some text!\n","\n","---\n","\n","\n","**TODO:** \n","\n","**DONE:**\n","* Create some cool output\n","\n","\n","\n","\n","Assuming everything has gone well, you should be able to run the main function in the scaffold code, using either your custom GRU cell or the built in layer, and see output something like this. I trained on the “lotr.txt” dataset, using chunk_length=200, hidden_size=100 for 2000 epochs. These are the results, along with the prime string:\n","\n","---\n","\n"," G:\n"," \n"," Gandalf was decrond. \n","'All have lord you. Forward the road at least walk this is stuff, and \n","went to the long grey housel-winding and kindled side was a sleep pleasuring, I do long \n","row hrough. In  \n","\n"," lo:\n"," \n"," lost death it. \n","'The last of the gatherings and take you,' said Aragorn, shining out of the Gate. \n","'Yes, as you there were remembaused to seen their pass, when? What \n","said here, such seven an the sear \n","\n"," lo:\n"," \n"," low, and frod to keepn \n","Came of their most. But here priced doubtless to an Sam up is \n","masters; he left hor as they are looked. And he could now the long to stout in the right fro horseless of \n","the like \n","\n"," I:\n"," \n"," I had been the \n","in his eyes with the perushed to lest, if then only the ring and the legended \n","of the less of the long they which as the \n","enders of Orcovered and smood, and the p \n","\n"," I:\n"," \n"," I they were not the lord of the hoomes. \n","Home already well from the Elves. And he sat strength, and we \n","housed out of the good of the days to the mountains from his perith. \n","\n","'Yess! Where though as if  \n","\n"," Th:\n"," \n"," There yarden \n","you would guard the hoor might. Far and then may was \n","croties, too began to see the drumbred many line \n","and was then hoard walk and they heart, and the chair of the \n","Ents of way, might was \n","\n"," G:\n"," \n"," Gandalf \n","been lat of less the round of the stump; both and seemed to the trees and perished they \n","lay are speered the less; and the wind the steep and have to she \n","precious. There was in the oonly went \n","\n"," wh:\n"," \n"," which went out of the door. \n","Hull the King and of the The days of his brodo \n","stumbler of the windard was a thing there, then it been shining langing \n","to him poor land. They hands; though they seemed ou \n","\n"," ra:\n"," \n"," rather,' have all the least deather \n","down of the truven beginning to the house of sunk. \n","'Nark shorts of the Eyes of the Gate your great nothing as Eret. \n","'I wander trust horn, and there were not, it  \n","\n"," I:\n"," \n"," I can have no mind \n","together! Where don't may had one may little blung \n","terrible to tales. And turn and Gandalf shall be not to as only the Cattring \n","not stopped great the out them forms. On they she lo \n","\n","---\n"]},{"cell_type":"code","execution_count":12,"metadata":{"id":"-nXFeCmdKodw","executionInfo":{"status":"ok","timestamp":1665967813366,"user_tz":360,"elapsed":6958,"user":{"displayName":"Grant White","userId":"00068657515062458882"}}},"outputs":[],"source":["import time\n","\n","n_epochs = 5000\n","print_every = 200\n","plot_every = 10\n","hidden_size = 200\n","n_layers = 3\n","lr = 0.001\n"," \n","decoder = RNN(n_characters, hidden_size, n_characters, n_layers)\n","decoder = decoder.cuda()\n","decoder_optimizer = torch.optim.Adam(decoder.parameters(), lr=lr)\n","criterion = nn.CrossEntropyLoss()\n"," \n","start = time.time()\n","all_losses = []\n","loss_avg = 0"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xKfozqw-6eqb","colab":{"base_uri":"https://localhost:8080/"},"outputId":"ff422d11-b7eb-403a-a0d9-624cf888e21e","executionInfo":{"status":"ok","timestamp":1665901591493,"user_tz":360,"elapsed":3907514,"user":{"displayName":"Grant White","userId":"00068657515062458882"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["[158.75932216644287 (200 4%) 2.4543]\n","Wh siat \n","\n","here whir. hald ariir, mir bir'le \n","\n","Air or hiwl phe hir \n","he Ros asferet Ghas thi \n","\n","ore fat \n","\n","[314.7109889984131 (400 8%) 2.2047]\n","When om with us ice turtares moon felcet shet herpoads theer and nomtend wond in theet and? \n","\n","I The  \n","\n","[470.7207565307617 (600 12%) 1.8075]\n","Wh. \n","\n","Bout do the wing seet we met of cabe at we had nout sady, \n","seid had of I meamen you. Nuch dear \n","\n","[628.1389856338501 (800 16%) 2.0303]\n","Wh we \n","soger water am pipe \n","the himsing and sure strime af the's the four laked and gay hinse boost  \n","\n","[783.7472183704376 (1000 20%) 2.0128]\n","Wh it was a tiyligets.' \n","But the ride, and crome again to me regore and the and \n","like fire onk was a \n","\n","[939.484744310379 (1200 24%) 1.7963]\n","Wh will all lave \n","\n","\n","\n","\n","\n","\n","\n","\n","\n","with home Ride than Light out with on the -main,' so not was been now fea \n","\n","[1095.2329308986664 (1400 28%) 1.6973]\n","Whing mare they flar so. At came seemed. Frido the Wells they heart. \n","\n","But were had can have that a  \n","\n","[1251.0313591957092 (1600 32%) 1.9251]\n","Wher take mess behends. 'Goldor,' said Golloked to Book into Gandalf one \n","pising to the Rohan lost,  \n","\n","[1406.8547894954681 (1800 36%) 1.5258]\n","Which a with me and get in the still \n","something company the sturing \n","washiled that it hall down he d \n","\n","[1562.6194150447845 (2000 40%) 1.4491]\n","Whated. You hund dut far the looked the Sready as say not \n","been, Then stide, of it a piting they \n","se \n","\n","[1718.3639135360718 (2200 44%) 1.7053]\n","What they woods, and the \n","saw speak blearing to a shall hold their faces now and better, uncome on t \n","\n","[1874.8882274627686 (2400 48%) 1.4928]\n","Where could me great \n","and a starros mighed have \n","seen as he said he words. Then a waure a darked the \n","\n","[2032.0778350830078 (2600 52%) 1.5519]\n","Wher was well, it ise \n","grey \n","a sidely that they bade is. 'I was not was not is negred with far espar \n","\n","[2188.0186727046967 (2800 56%) 1.4701]\n","Where and some him, and the, \n","under strength if Singing of him. The \n","passed the esach pasing shoulde \n","\n","[2344.429327726364 (3000 60%) 1.3841]\n","What foot along ate he dig time take to the landor, the \n","fear to have do not life of the valley. \n","\n","T \n","\n","[2500.501218557358 (3200 64%) 2.0189]\n","What he seemed was at with sromple was a littlement of it is going and \n","palion. \n","\n","xeathed this redda \n","\n","[2656.4995222091675 (3400 68%) 1.3380]\n","Where had road, as he was \n","that of the land in the feet helding.\" \n","\n","All then they seemed on dare. An \n","\n","[2813.0800733566284 (3600 72%) 1.4090]\n","Where was a strong, and \n","and for the thickets were is the shadow a who was flwed him to the mouth th \n","\n","[2969.413016319275 (3800 76%) 1.4574]\n","Which the \n","narroon, it should not own are were deep. He shadow excapped \n","day Sam, about hobbits and  \n","\n","[3126.17240524292 (4000 80%) 1.7964]\n","White that a looked as this had that \n","the Down that he discourced to the halls walls. Quickly cried  \n","\n","[3282.096438884735 (4200 84%) 1.2556]\n","Whose his seen that the \n","irad of the river still high \n","with many time. It tussy in the valleyed his  \n","\n","[3439.2159469127655 (4400 88%) 1.4512]\n","Wh ever Sam's enough good, however while clifbed \n","never stold yet out of caint there is tired the ru \n","\n","[3596.333924293518 (4600 92%) 1.3470]\n","Whilid, and the looking and and he known the spent on \n","his fair lies, the wind of the fields quiet w \n","\n","[3752.468332052231 (4800 96%) 1.4294]\n","While in the hearth of the \n","Door the man and tlent. \n","But if the matter to see little was \n","dreambad,  \n","\n","[3908.5139484405518 (5000 100%) 1.2659]\n","While the sun deep of the way to sudden \n","dark under the voured. \n","\n","There was the wrather and slopes u \n","\n"]}],"source":["# n_epochs = 2000\n","for epoch in range(1, n_epochs + 1):\n","    loss_ = train(*random_training_set())       \n","    loss_avg += loss_\n","\n","    if epoch % print_every == 0:\n","        print('[%s (%d %d%%) %.4f]' % (time.time() - start, epoch, epoch / n_epochs * 100, loss_))\n","        print(evaluate('Wh', 100), '\\n')\n","\n","    if epoch % plot_every == 0:\n","        all_losses.append(loss_avg / plot_every)\n","        loss_avg = 0"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ee0so6aKJ5L8","executionInfo":{"status":"ok","timestamp":1665901591668,"user_tz":360,"elapsed":199,"user":{"displayName":"Grant White","userId":"00068657515062458882"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"bd54d771-d404-4f93-c8f2-7433d5e1196c"},"outputs":[{"output_type":"stream","name":"stdout","text":[" ra\n"," raned through the \n","ruses with the ports. But not suddenly rain the East of your heart with \n","the trair more last. There was they has enough not back \n","after a great dread night has very day flade a bea \n","\n"," he\n"," hed lies, tom right while their eyes and planded the \n","Serves whistented and stood the high and the rustarded his slow, and has heard up. \n","\n","Suddenly to desirite by the shadows of the turred the bares  \n","\n"," Th\n"," Thisted us every clear firest of her to the clistly \n","banning fire of the brooct. Pippin listened the others, far \n","which the King in the great, her eyes and strunger they seemed the else and by the ma \n","\n"," ra\n"," raded by the road. He was shall speak of the stopped \n","pouring. There was not to be rung and pale was for so stand they same the \n","courter. Frodo start in the welk place for the stone deep his cried hi \n","\n"," G\n"," Grandless, two the dark some \n","clessed their led of the soon on the great hole-strode deep, as he said the \n","Mark of the Brandir with the dell at the horse. Tomer to be road, and undell \n","one others wil \n","\n"," ra\n"," rads truth the first fire last from the ring dread forll of \n","the clouds that in a third, as will be crawiness place and the \n","world with the long with lean saw a little presetter from the things or ti \n","\n"," Th\n"," This fallen seemed by you do you think, if you must have \n","was stood Frodo. But I has no dear after one and short is not the door was not \n","dise at and try turn up the men. I am suppose from his putter \n","\n"," wh\n"," where was brought the scresent, they little bare. Tomove the sun in \n","the steed and Biltone far forgotten speak drew them, and when the lange harding of the water by field and of his paths \n","there in t \n","\n"," ra\n"," raled with \n","the encants. There is you sent with them that has put any little is not rue essenger and \n","still all afterth just see you stry to what \n","there some word world. Well, what you know even to g \n","\n"," he\n"," her, days now any at the great suddenly them and pale \n","calleys. \n","\n","They will not them, and where last stood his neither \n","precipt, there life by hobbits they say any weather falling. \n","\n","'I shall be that \n","\n"]}],"source":["for i in range(10):\n","    start_strings = [\" Th\", \" wh\", \" he\", \" I \", \" ca\", \" G\", \" lo\", \" ra\"]\n","    start = random.randint(0,len(start_strings)-1)\n","    print(start_strings[start])\n","    #   all_characters.index(string[c])\n","    print(evaluate(start_strings[start], 200), '\\n')"]},{"cell_type":"markdown","metadata":{"id":"YJhgDc2IauPE"},"source":["---\n","\n","## Part 6: Generate output on a different dataset\n","\n","---\n","\n","**TODO:**\n","\n","**DONE:**\n","\n","* Choose a textual dataset. Here are some [text datasets](https://www.kaggle.com/datasets?tags=14104-text+data%2C13205-text+mining) from Kaggle \n","\n","* Generate some decent looking results and evaluate your model's performance (say what it did well / not so well)S\n","\n"]},{"cell_type":"code","source":["file = unidecode.unidecode(open('./normal_speech.txt').read())\n","file_len = len(file)\n","print('file_len =', file_len)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uXZo5PnJPa6L","executionInfo":{"status":"ok","timestamp":1665967581862,"user_tz":360,"elapsed":937,"user":{"displayName":"Grant White","userId":"00068657515062458882"}},"outputId":"28e380c2-9b1f-4c39-a446-c70c7876e999"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["file_len = 249679\n"]}]},{"cell_type":"code","source":["for epoch in range(1, n_epochs + 1):\n","    loss_ = train(*random_training_set())       \n","    loss_avg += loss_\n","\n","    if epoch % print_every == 0:\n","        print('[%s (%d %d%%) %.4f]' % (time.time() - start, epoch, epoch / n_epochs * 100, loss_))\n","        print(evaluate('Wh', 100), '\\n')\n","\n","    if epoch % plot_every == 0:\n","        all_losses.append(loss_avg / plot_every)\n","        loss_avg = 0"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vey_qliEPsJ3","outputId":"b0ed55b1-76a4-482b-9080-a4304850d78d","executionInfo":{"status":"ok","timestamp":1665969826041,"user_tz":360,"elapsed":1884241,"user":{"displayName":"Grant White","userId":"00068657515062458882"}}},"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["[204.26637864112854 (200 4%) 2.3602]\n","Whanl nnoul alid lhol, iny.e at way yot. that.n hanr  re he'd  ne tise rous he ousinked a fom  inll  \n","\n","[279.5081624984741 (400 8%) 2.2789]\n","Wh.\n","what thaces sa\n","tout'.    that ham.    i do you shat proulling?\n","int's forongens  geost?     it he \n","\n","[354.8873829841614 (600 12%) 1.5517]\n","Wh sass lot sfole pealnt.\n","ouble wiml to we worter you wlinktise picf as tusleets?\n","beatitte it it bei \n","\n","[430.24724316596985 (800 16%) 1.8346]\n","Wh a lots bet a book you some sule to chy only saiker i nem thout.\n","i can cousse see was do to setter \n","\n","[505.6461420059204 (1000 20%) 1.7270]\n","Wher proog to say didn't hat ahe wemtor, it's here could?  did you does fun tome mut think you will  \n","\n","[581.0196619033813 (1200 24%) 1.4851]\n","Whed of him necclapfed it.\n","it's not? i think i wait with the fines ny.\n","i wold be the bill blees me u \n","\n","[656.5073094367981 (1400 28%) 1.3670]\n","Wher do the droft to have the like.   i got it are new that new of the card.\n","that time are your you  \n","\n","[731.7285611629486 (1600 32%) 1.4623]\n","Wher right a right.\n","it stalt is true to sect eve do?  me tree diend amould ever on.\n","what was to say  \n","\n","[807.1222100257874 (1800 36%) 1.7390]\n","Wher then is the swould see for's here or a lice that.    no. more the plime the they're tell geap t \n","\n","[882.5537145137787 (2000 40%) 1.3343]\n","What you came a nice the got fun teen.   it was not you think it? you're that it spad. i've tolk exo \n","\n","[957.9296202659607 (2200 44%) 1.3634]\n","Where proble be for on your took to be thoeeseld on a fur?\n","why're did you do?    what's on the right \n","\n","[1033.06347322464 (2400 48%) 1.0660]\n","Whable parent. that else the is a great just say.\n","that's not mean.\n","not?  that's a good the eat thing \n","\n","[1108.384583234787 (2600 52%) 1.2165]\n","Wher, but it's too bobbbute to go myself.\n","i thought a car with i wish i feel been to just into see t \n","\n","[1183.7470769882202 (2800 56%) 1.2454]\n","Whore?\n","did you dight?   it's going to be good, like than see. yes, yes, i don't want to go to sounds \n","\n","[1259.3621439933777 (3000 60%) 1.2063]\n","Wher ham a base.    i wriend, it's good.  so do you don't was a rementring into a buyer.\n","i think i d \n","\n","[1334.6489117145538 (3200 64%) 1.3305]\n","Whave ancost, we wait.    i don't underniany to don't get a come of you.\n","i haven't will you know a h \n","\n","[1409.8700444698334 (3400 68%) 1.2747]\n","Whan.\n","thener said that was cicull pails.    but it's hand ruin buil cherse than.\n","you're missing the  \n","\n","[1485.140149116516 (3600 72%) 1.0706]\n","Wh and one though.\n","i wish it happened to stop thinking, i do. i had to do you have out.\n","it's the ble \n","\n","[1560.76486825943 (3800 76%) 1.6182]\n","When are school coff?\n","what happened?   that's time at the shots and people of the streets.\n","from at z \n","\n","[1636.0584318637848 (4000 80%) 1.2598]\n","Where ar on my with the one with a new new falling to see the table.\n","i was bought a police to did th \n","\n","[1711.4171073436737 (4200 84%) 1.1500]\n","Whands.    do you toarther would go to preper?\n","so do it all the big was a first?    thank you.\n","i hav \n","\n","[1786.4863169193268 (4400 88%) 1.3229]\n","Wher.\n","i hate to be dangerous.  if that may a dictionary.\n","i was so, someonce also tomorry.   forever  \n","\n","[1861.6815967559814 (4600 92%) 1.4282]\n","Where is in schools are in the fircable. why not of or?\n","you make it.    but he's the worth the weeke \n","\n","[1937.2308332920074 (4800 96%) 0.9690]\n","Whing doing to church.  what down?\n","are the carted?    i couple of what movie traiil is a good out to \n","\n","[2012.5045583248138 (5000 100%) 1.0809]\n","Where a couple of it.\n","what? of course.\n","my sigure, it has a good.   what? the only, too, it's only $1 \n","\n"]}]},{"cell_type":"code","source":["for i in range(10):\n","    start_strings = [\" What is your name?\",\n","                     \" My name is\",\n","                     \" I feel\",\n","                     \" Why do\"]\n","    start = random.randint(0,len(start_strings)-1)\n","    print(start_strings[start])\n","    #   all_characters.index(string[c])\n","    print(evaluate(start_strings[start], 200), '\\n')"],"metadata":{"id":"fEkuS3NvPvvI","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1665970082311,"user_tz":360,"elapsed":1732,"user":{"displayName":"Grant White","userId":"00068657515062458882"}},"outputId":"fd6f2344-847d-43c0-d4f6-d1c9a9820ff1"},"execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":[" My name is\n"," My name is.\n","they should people warmel money in a saladly.   i need a all of money.  a can see it bases and his early.\n","let's been a nice a bater. that's not a really don't all his gragice.\n","every rabbe \n","\n"," My name is\n"," My name isand?\n","what are you like? old at the houses different pocket.\n","but the weather was a bark about a lot.    what kinding you had her baby one?\n","does man we got to be?    a bear for a money.   i t \n","\n"," Why do\n"," Why dor.\n","i was take that at some into the shoking.    what else i shouldn't rain like?\n","what do you mean?   i don't free your hours, but you've use for my friend.\n","but you're right a bank something.    \n","\n"," What is your name?\n"," What is your name?  have years about $8, and have to go.\n","because they are carts at the sholling to make a would fing.   well, you're right right?\n","what hard about the way?    okay, more that an outer? \n","\n"," I feel\n"," I feely eat the collell parts.\n","but can be weather is so strupe.    the streets weal blow his storatimans.\n","the tall go out election if he heard of the invite storands.    they are back all people for  \n","\n"," I feel\n"," I feel from there.\n","it's hurt again.  it's been hot right not.\n","it would after them outside.  well, it all the strupe outside.\n","i feel line the instrundlatest.  she been smatbelt a cusband on the night. \n","\n"," I feel\n"," I feel back used to my dag so much strame.\n","but they say that you tise the poble. i stopd your house.\n","that's a racking about a half that so back ya.  it wasn't bathrooms.\n","i cill the bathrooms to my ca \n","\n"," Why do\n"," Why dors.   i'll buy that die.\n","but it's a third would be.   they seen that sauden actom ago.\n","she looks a chame tall bad baby shirt. he's some money.\n","the cat shopped.    i am show it rains.\n","i have to  \n","\n"," What is your name?\n"," What is your name?   yes, i had to blay is early.\n","and that's at the movie.  maybe i was blinking.   i didn't blow for fat carry..\n","i could fun. i think just brings like to blue a fall to buy a nice fu \n","\n"," My name is\n"," My name ise.\n","well, i had to blow that it different.    it was hers.\n","the would be and stubpen.   i have to live the money for the time that my friends like for this hard.\n","i don't carry honey.    yes,  \n","\n"]}]},{"cell_type":"markdown","source":["I wanted to train my model on more real, modern speech. The output is okay. A lot of phrases are good, but sentences don't make sense. All the words look good though. I think with a little more training the results would be pretty good."],"metadata":{"id":"wpJ2SCbzjwAK"}},{"cell_type":"code","source":[],"metadata":{"id":"6cpgjjKzkgt1"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"collapsed_sections":[],"provenance":[],"machine_shape":"hm"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.1"},"pycharm":{"stem_cell":{"cell_type":"raw","metadata":{"collapsed":false},"source":[]}},"accelerator":"GPU","gpuClass":"premium"},"nbformat":4,"nbformat_minor":0}